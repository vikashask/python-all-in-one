{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¼ Complete Pandas Mastery: From Zero to Hero\n",
        "\n",
        "**A Comprehensive, Hands-On Guide to Data Analysis with Pandas**\n",
        "\n",
        "---\n",
        "\n",
        "## Course Overview\n",
        "\n",
        "| Module | Topic | Level |\n",
        "|--------|-------|-------|\n",
        "| 1 | Introduction & Setup | Beginner |\n",
        "| 2 | Core Data Structures (Series & DataFrame) | Beginner |\n",
        "| 3 | Data Selection & Indexing | Beginner |\n",
        "| 4 | Data Cleaning | Intermediate |\n",
        "| 5 | Data Transformation | Intermediate |\n",
        "| 6 | Aggregation & Grouping | Intermediate |\n",
        "| 7 | Merging & Joining | Intermediate |\n",
        "| 8 | Reshaping Data | Advanced |\n",
        "| 9 | Time Series | Advanced |\n",
        "| 10 | I/O Operations | Intermediate |\n",
        "| 11 | Performance Optimization | Advanced |\n",
        "| 12 | Real-World Project | Advanced |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 1: Introduction & Setup\n",
        "\n",
        "## What is Pandas?\n",
        "\n",
        "**Pandas** is Python's most powerful library for data manipulation and analysis. The name comes from \"Panel Data\" (an econometrics term).\n",
        "\n",
        "### Why Use Pandas?\n",
        "- **Easy Data Handling**: Read/write CSV, Excel, SQL, JSON\n",
        "- **Powerful Selection**: Filter and slice data intuitively\n",
        "- **Data Cleaning**: Handle missing data and duplicates\n",
        "- **Data Transformation**: Reshape, merge, group, aggregate\n",
        "- **Time Series**: Built-in date/time support\n",
        "\n",
        "### Installation\n",
        "```bash\n",
        "pip install pandas\n",
        "# OR\n",
        "conda install pandas\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports - Run this first!\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(\"âœ… Ready to learn Pandas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 2: Core Data Structures\n",
        "\n",
        "Pandas has two primary data structures:\n",
        "\n",
        "| Structure | Dimensions | Description | Analogy |\n",
        "|-----------|------------|-------------|---------|\n",
        "| **Series** | 1D | A single column of data | Excel column |\n",
        "| **DataFrame** | 2D | Table with rows & columns | Excel spreadsheet |\n",
        "\n",
        "## 2.1 Series - One-Dimensional Data\n",
        "\n",
        "A **Series** is a 1D labeled array holding any data type with:\n",
        "- **Index** (labels for each element)\n",
        "- **Values** (the actual data)\n",
        "- **Data type** (dtype)\n",
        "- **Name** (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CREATING A SERIES - Multiple Methods\n",
        "# ========================================\n",
        "\n",
        "# Method 1: From a Python list\n",
        "temperatures = pd.Series([22, 25, 28, 30, 27])\n",
        "print(\"Method 1 - From list:\")\n",
        "print(temperatures)\n",
        "print(f\"Type: {type(temperatures)}\\n\")\n",
        "\n",
        "# Method 2: With custom index (labels)\n",
        "temperatures = pd.Series(\n",
        "    data=[22, 25, 28, 30, 27],\n",
        "    index=['Mon', 'Tue', 'Wed', 'Thu', 'Fri'],\n",
        "    name='Daily Temperature (Â°C)'\n",
        ")\n",
        "print(\"Method 2 - With custom index:\")\n",
        "print(temperatures)\n",
        "print()\n",
        "\n",
        "# Method 3: From a dictionary (keys become index)\n",
        "population = pd.Series({\n",
        "    'India': 1400000000,\n",
        "    'China': 1410000000,\n",
        "    'USA': 331000000,\n",
        "})\n",
        "print(\"Method 3 - From dictionary:\")\n",
        "print(population)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# SERIES ATTRIBUTES & METHODS\n",
        "# ========================================\n",
        "\n",
        "sales = pd.Series(\n",
        "    [1200, 1500, 1100, 1800, 1600],\n",
        "    index=['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n",
        "    name='Monthly Sales'\n",
        ")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"SERIES ATTRIBUTES\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Values: {sales.values}\")\n",
        "print(f\"Index: {sales.index.tolist()}\")\n",
        "print(f\"Data type: {sales.dtype}\")\n",
        "print(f\"Name: {sales.name}\")\n",
        "print(f\"Shape: {sales.shape}\")\n",
        "print(f\"Size: {sales.size}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"SERIES STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Sum: {sales.sum()}\")\n",
        "print(f\"Mean: {sales.mean():.2f}\")\n",
        "print(f\"Min: {sales.min()} | Max: {sales.max()}\")\n",
        "print(f\"\\nQuick Stats:\\n{sales.describe()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 DataFrame - Two-Dimensional Data\n",
        "\n",
        "A **DataFrame** is a 2D labeled data structure - like a spreadsheet or SQL table.\n",
        "- Has **row index** and **column names**\n",
        "- Each column can have different data types\n",
        "- Size is mutable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CREATING A DATAFRAME - Multiple Methods\n",
        "# ========================================\n",
        "\n",
        "# Method 1: From a dictionary of lists (MOST COMMON)\n",
        "employees = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
        "    'department': ['Sales', 'IT', 'HR', 'IT', 'Sales'],\n",
        "    'age': [28, 35, 42, 31, 29],\n",
        "    'salary': [55000, 72000, 58000, 68000, 52000]\n",
        "})\n",
        "\n",
        "print(\"DataFrame from dictionary of lists:\")\n",
        "print(employees)\n",
        "print(f\"\\nShape: {employees.shape}\")\n",
        "print(f\"Columns: {employees.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DATAFRAME QUICK OVERVIEW METHODS\n",
        "# ========================================\n",
        "\n",
        "print(\"--- df.head(3) - First 3 rows ---\")\n",
        "print(employees.head(3))\n",
        "\n",
        "print(\"\\n--- df.tail(2) - Last 2 rows ---\")\n",
        "print(employees.tail(2))\n",
        "\n",
        "print(\"\\n--- df.info() - DataFrame info ---\")\n",
        "employees.info()\n",
        "\n",
        "print(\"\\n--- df.describe() - Statistics ---\")\n",
        "print(employees.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 3: Data Selection & Indexing\n",
        "\n",
        "**This is the most important module!** Mastering selection is key to working with data.\n",
        "\n",
        "## Selection Methods Overview\n",
        "\n",
        "| Method | Description | Example |\n",
        "|--------|-------------|---------|\n",
        "| `df['col']` | Select single column | `df['name']` |\n",
        "| `df[['col1', 'col2']]` | Select multiple columns | `df[['name', 'age']]` |\n",
        "| `df.loc[]` | Select by **label** | `df.loc[0, 'name']` |\n",
        "| `df.iloc[]` | Select by **position** | `df.iloc[0, 0]` |\n",
        "| `df[condition]` | Boolean filtering | `df[df['age'] > 30]` |\n",
        "\n",
        "### Memory Trick:\n",
        "- **loc** = **L**abel-based (\"**L**ocate by name\")\n",
        "- **iloc** = **I**nteger-based (\"**I**ndex position\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample DataFrame for selection examples\n",
        "df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
        "    'department': ['Sales', 'IT', 'HR', 'IT', 'Sales', 'HR'],\n",
        "    'age': [28, 35, 42, 31, 29, 45],\n",
        "    'salary': [55000, 72000, 58000, 68000, 52000, 62000],\n",
        "    'years_exp': [3, 8, 15, 6, 4, 20]\n",
        "})\n",
        "print(\"Sample DataFrame:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# COLUMN SELECTION\n",
        "# ========================================\n",
        "\n",
        "# Single column (returns Series)\n",
        "print(\"=== Single Column Selection ===\")\n",
        "print(df['name'])\n",
        "print(f\"Type: {type(df['name'])}\\n\")\n",
        "\n",
        "# Multiple columns (returns DataFrame)\n",
        "print(\"=== Multiple Column Selection ===\")\n",
        "print(df[['name', 'salary', 'department']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# .loc[] - LABEL-BASED SELECTION\n",
        "# ========================================\n",
        "\n",
        "print(\"=== .loc[] - Label-Based Selection ===\\n\")\n",
        "\n",
        "# Single row by label\n",
        "print(\"1. Single row: df.loc[0]\")\n",
        "print(df.loc[0])\n",
        "\n",
        "print(\"\\n2. Specific cell: df.loc[1, 'name']\")\n",
        "print(df.loc[1, 'name'])\n",
        "\n",
        "print(\"\\n3. Multiple rows & cols: df.loc[0:2, ['name', 'salary']]\")\n",
        "print(df.loc[0:2, ['name', 'salary']])  # Note: 2 IS included with loc!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# .iloc[] - POSITION-BASED SELECTION\n",
        "# ========================================\n",
        "\n",
        "print(\"=== .iloc[] - Position-Based Selection ===\\n\")\n",
        "\n",
        "# Single row by position\n",
        "print(\"1. First row: df.iloc[0]\")\n",
        "print(df.iloc[0])\n",
        "\n",
        "print(\"\\n2. Specific cell (row 1, col 0): df.iloc[1, 0]\")\n",
        "print(df.iloc[1, 0])\n",
        "\n",
        "print(\"\\n3. Rows 0-2, cols 0-2: df.iloc[0:3, 0:3]\")\n",
        "print(df.iloc[0:3, 0:3])  # Note: 3 is NOT included with iloc!\n",
        "\n",
        "print(\"\\n4. Last row: df.iloc[-1]\")\n",
        "print(df.iloc[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# BOOLEAN INDEXING (FILTERING)\n",
        "# ========================================\n",
        "\n",
        "print(\"=== Boolean Indexing ===\\n\")\n",
        "\n",
        "# Single condition\n",
        "print(\"1. Employees over 30:\")\n",
        "print(df[df['age'] > 30])\n",
        "\n",
        "# Multiple conditions with AND (&)\n",
        "print(\"\\n2. IT dept AND salary > 65000:\")\n",
        "print(df[(df['department'] == 'IT') & (df['salary'] > 65000)])\n",
        "\n",
        "# Multiple conditions with OR (|)\n",
        "print(\"\\n3. Sales OR HR department:\")\n",
        "print(df[(df['department'] == 'Sales') | (df['department'] == 'HR')])\n",
        "\n",
        "# Using isin() for multiple values\n",
        "print(\"\\n4. Using isin() - Sales or HR:\")\n",
        "print(df[df['department'].isin(['Sales', 'HR'])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 4: Data Cleaning\n",
        "\n",
        "Real-world data is messy! This module teaches you how to handle:\n",
        "- **Missing values** (NaN)\n",
        "- **Duplicate rows**\n",
        "- **Data type issues**\n",
        "- **String cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# HANDLING MISSING DATA (NaN)\n",
        "# ========================================\n",
        "\n",
        "# Create data with missing values\n",
        "df_missing = pd.DataFrame({\n",
        "    'A': [1, 2, None, 4, 5],\n",
        "    'B': [None, 2, 3, None, 5],\n",
        "    'C': [1, 2, 3, 4, None]\n",
        "})\n",
        "print(\"DataFrame with missing values:\")\n",
        "print(df_missing)\n",
        "\n",
        "print(\"\\n=== Detecting Missing Values ===\")\n",
        "print(f\"\\nMissing per column:\\n{df_missing.isna().sum()}\")\n",
        "print(f\"\\nTotal missing: {df_missing.isna().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# HANDLING MISSING VALUES\n",
        "# ========================================\n",
        "\n",
        "print(\"=== Dropping Missing Values ===\")\n",
        "print(\"\\n1. Drop rows with ANY missing:\")\n",
        "print(df_missing.dropna())\n",
        "\n",
        "print(\"\\n2. Drop only if ALL values missing:\")\n",
        "print(df_missing.dropna(how='all'))\n",
        "\n",
        "print(\"\\n=== Filling Missing Values ===\")\n",
        "print(\"\\n3. Fill with 0:\")\n",
        "print(df_missing.fillna(0))\n",
        "\n",
        "print(\"\\n4. Fill with column mean:\")\n",
        "print(df_missing.fillna(df_missing.mean()))\n",
        "\n",
        "print(\"\\n5. Forward fill (use previous value):\")\n",
        "print(df_missing.ffill())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# HANDLING DUPLICATES\n",
        "# ========================================\n",
        "\n",
        "df_dup = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Bob', 'Charlie', 'Alice'],\n",
        "    'age': [28, 35, 35, 42, 28],\n",
        "    'city': ['NYC', 'LA', 'LA', 'Chicago', 'NYC']\n",
        "})\n",
        "print(\"DataFrame with duplicates:\")\n",
        "print(df_dup)\n",
        "\n",
        "print(f\"\\n=== Detecting Duplicates ===\")\n",
        "print(f\"Duplicate rows: {df_dup.duplicated().sum()}\")\n",
        "print(f\"\\nView duplicates:\\n{df_dup[df_dup.duplicated(keep=False)]}\")\n",
        "\n",
        "print(\"\\n=== Removing Duplicates ===\")\n",
        "print(df_dup.drop_duplicates())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STRING OPERATIONS\n",
        "# ========================================\n",
        "\n",
        "df_str = pd.DataFrame({\n",
        "    'name': ['  Alice Smith  ', 'BOB JONES', 'charlie brown'],\n",
        "    'email': ['ALICE@EMAIL.COM', 'bob@email.com', 'Charlie@Email.Com']\n",
        "})\n",
        "print(\"Original:\")\n",
        "print(df_str)\n",
        "\n",
        "print(\"\\n=== String Cleaning ===\")\n",
        "print(\"\\n1. Strip whitespace:\")\n",
        "print(df_str['name'].str.strip())\n",
        "\n",
        "print(\"\\n2. Lowercase:\")\n",
        "print(df_str['email'].str.lower())\n",
        "\n",
        "print(\"\\n3. Title case:\")\n",
        "print(df_str['name'].str.strip().str.title())\n",
        "\n",
        "print(\"\\n4. Contains check:\")\n",
        "print(df_str['email'].str.lower().str.contains('email'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 5: Data Transformation\n",
        "\n",
        "Learn how to modify, add, and transform your data using:\n",
        "- Adding/removing columns\n",
        "- `apply()`, `map()`, `transform()`\n",
        "- Sorting and ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ADDING & MODIFYING COLUMNS\n",
        "# ========================================\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'salary': [55000, 72000, 58000, 68000],\n",
        "    'bonus_pct': [0.10, 0.15, 0.08, 0.12]\n",
        "})\n",
        "\n",
        "# Method 1: Direct calculation\n",
        "df['bonus'] = df['salary'] * df['bonus_pct']\n",
        "df['total_comp'] = df['salary'] + df['bonus']\n",
        "\n",
        "# Method 2: Conditional with np.where\n",
        "df['level'] = np.where(df['salary'] > 60000, 'Senior', 'Junior')\n",
        "\n",
        "print(\"After adding columns:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# APPLY, MAP, AND TRANSFORM\n",
        "# ========================================\n",
        "\n",
        "print(\"=== .apply() - Apply function to Series/DataFrame ===\\n\")\n",
        "\n",
        "# Apply to column\n",
        "def format_salary(x):\n",
        "    return f\"${x:,.0f}\"\n",
        "\n",
        "df['salary_fmt'] = df['salary'].apply(format_salary)\n",
        "print(\"1. Apply function to column:\")\n",
        "print(df[['name', 'salary', 'salary_fmt']])\n",
        "\n",
        "# Apply with lambda\n",
        "df['name_upper'] = df['name'].apply(lambda x: x.upper())\n",
        "print(\"\\n2. Apply lambda:\")\n",
        "print(df[['name', 'name_upper']])\n",
        "\n",
        "# Apply row-wise (axis=1)\n",
        "df['summary'] = df.apply(lambda row: f\"{row['name']}: {row['level']}\", axis=1)\n",
        "print(\"\\n3. Apply row-wise:\")\n",
        "print(df[['summary']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# SORTING & RANKING\n",
        "# ========================================\n",
        "\n",
        "df_sort = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'age': [28, 35, 42, 31],\n",
        "    'salary': [55000, 72000, 58000, 68000]\n",
        "})\n",
        "\n",
        "print(\"=== Sorting ===\")\n",
        "print(\"\\n1. Sort by salary (ascending):\")\n",
        "print(df_sort.sort_values('salary'))\n",
        "\n",
        "print(\"\\n2. Sort by salary (descending):\")\n",
        "print(df_sort.sort_values('salary', ascending=False))\n",
        "\n",
        "print(\"\\n3. Sort by multiple columns:\")\n",
        "print(df_sort.sort_values(['age', 'salary'], ascending=[True, False]))\n",
        "\n",
        "print(\"\\n=== Ranking ===\")\n",
        "df_sort['salary_rank'] = df_sort['salary'].rank(ascending=False)\n",
        "print(df_sort)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 6: Aggregation & Grouping\n",
        "\n",
        "**GroupBy** is one of Pandas' most powerful features. It follows the **Split-Apply-Combine** pattern:\n",
        "\n",
        "1. **Split**: Divide data into groups\n",
        "2. **Apply**: Apply a function to each group\n",
        "3. **Combine**: Combine results into a new structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# GROUPBY OPERATIONS\n",
        "# ========================================\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'department': ['Sales', 'IT', 'HR', 'IT', 'Sales', 'HR'],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank'],\n",
        "    'salary': [55000, 72000, 58000, 68000, 52000, 62000],\n",
        "    'years': [3, 8, 15, 6, 4, 20]\n",
        "})\n",
        "print(\"Sample Data:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\n=== Basic GroupBy ===\")\n",
        "print(\"\\n1. Mean salary by department:\")\n",
        "print(df.groupby('department')['salary'].mean())\n",
        "\n",
        "print(\"\\n2. Multiple aggregations:\")\n",
        "print(df.groupby('department')['salary'].agg(['mean', 'min', 'max', 'count']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ADVANCED GROUPBY\n",
        "# ========================================\n",
        "\n",
        "print(\"=== Advanced GroupBy ===\")\n",
        "\n",
        "# Different aggregations for different columns\n",
        "print(\"\\n1. Different agg per column:\")\n",
        "result = df.groupby('department').agg({\n",
        "    'salary': ['mean', 'sum'],\n",
        "    'years': 'max',\n",
        "    'name': 'count'\n",
        "})\n",
        "print(result)\n",
        "\n",
        "# Named aggregations (cleaner output)\n",
        "print(\"\\n2. Named aggregations:\")\n",
        "result = df.groupby('department').agg(\n",
        "    avg_salary=('salary', 'mean'),\n",
        "    total_salary=('salary', 'sum'),\n",
        "    max_years=('years', 'max'),\n",
        "    employee_count=('name', 'count')\n",
        ")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# PIVOT TABLES\n",
        "# ========================================\n",
        "\n",
        "sales_data = pd.DataFrame({\n",
        "    'region': ['East', 'East', 'West', 'West', 'East', 'West'],\n",
        "    'product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'sales': [100, 150, 200, 175, 120, 190],\n",
        "    'quarter': ['Q1', 'Q1', 'Q1', 'Q1', 'Q2', 'Q2']\n",
        "})\n",
        "\n",
        "print(\"Sales Data:\")\n",
        "print(sales_data)\n",
        "\n",
        "print(\"\\n=== Pivot Table ===\")\n",
        "pivot = pd.pivot_table(\n",
        "    sales_data,\n",
        "    values='sales',\n",
        "    index='region',\n",
        "    columns='product',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "print(pivot)\n",
        "\n",
        "print(\"\\n=== Pivot with margins (totals) ===\")\n",
        "pivot_margins = pd.pivot_table(\n",
        "    sales_data,\n",
        "    values='sales',\n",
        "    index='region',\n",
        "    columns='product',\n",
        "    aggfunc='sum',\n",
        "    margins=True  # Adds row/column totals\n",
        ")\n",
        "print(pivot_margins)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 7: Merging & Joining\n",
        "\n",
        "Combine multiple DataFrames using:\n",
        "- **concat()**: Stack DataFrames vertically or horizontally\n",
        "- **merge()**: SQL-style joins (inner, left, right, outer)\n",
        "- **join()**: Join on index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# CONCATENATION\n",
        "# ========================================\n",
        "\n",
        "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
        "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
        "\n",
        "print(\"df1:\")\n",
        "print(df1)\n",
        "print(\"\\ndf2:\")\n",
        "print(df2)\n",
        "\n",
        "print(\"\\n=== Vertical concat (stack rows) ===\")\n",
        "print(pd.concat([df1, df2], ignore_index=True))\n",
        "\n",
        "print(\"\\n=== Horizontal concat (side by side) ===\")\n",
        "print(pd.concat([df1, df2], axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# MERGE - SQL-STYLE JOINS\n",
        "# ========================================\n",
        "\n",
        "employees = pd.DataFrame({\n",
        "    'emp_id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
        "    'dept_id': [10, 20, 10, 30]\n",
        "})\n",
        "\n",
        "departments = pd.DataFrame({\n",
        "    'dept_id': [10, 20, 40],\n",
        "    'dept_name': ['Sales', 'IT', 'HR']\n",
        "})\n",
        "\n",
        "print(\"Employees:\")\n",
        "print(employees)\n",
        "print(\"\\nDepartments:\")\n",
        "print(departments)\n",
        "\n",
        "print(\"\\n=== INNER JOIN (only matching) ===\")\n",
        "print(pd.merge(employees, departments, on='dept_id', how='inner'))\n",
        "\n",
        "print(\"\\n=== LEFT JOIN (all from left) ===\")\n",
        "print(pd.merge(employees, departments, on='dept_id', how='left'))\n",
        "\n",
        "print(\"\\n=== OUTER JOIN (all from both) ===\")\n",
        "print(pd.merge(employees, departments, on='dept_id', how='outer'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 8: Reshaping Data\n",
        "\n",
        "Transform between **wide** and **long** formats:\n",
        "- **Wide format**: Each variable has its own column\n",
        "- **Long format**: Variables stacked in rows (tidy data)\n",
        "\n",
        "| Function | Description |\n",
        "|----------|-------------|\n",
        "| `melt()` | Wide â†’ Long |\n",
        "| `pivot()` | Long â†’ Wide |\n",
        "| `stack()` | Columns â†’ Rows |\n",
        "| `unstack()` | Rows â†’ Columns |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# MELT - WIDE TO LONG\n",
        "# ========================================\n",
        "\n",
        "# Wide format data\n",
        "wide_df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob'],\n",
        "    'math': [90, 85],\n",
        "    'science': [88, 92],\n",
        "    'english': [95, 78]\n",
        "})\n",
        "print(\"Wide format:\")\n",
        "print(wide_df)\n",
        "\n",
        "# Melt to long format\n",
        "long_df = pd.melt(\n",
        "    wide_df,\n",
        "    id_vars=['name'],        # Keep these columns\n",
        "    value_vars=['math', 'science', 'english'],  # Melt these\n",
        "    var_name='subject',      # Name for variable column\n",
        "    value_name='score'       # Name for value column\n",
        ")\n",
        "print(\"\\nLong format (melted):\")\n",
        "print(long_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# PIVOT - LONG TO WIDE\n",
        "# ========================================\n",
        "\n",
        "# Convert long back to wide\n",
        "pivoted = long_df.pivot(\n",
        "    index='name',\n",
        "    columns='subject',\n",
        "    values='score'\n",
        ")\n",
        "print(\"Pivoted back to wide:\")\n",
        "print(pivoted)\n",
        "\n",
        "# Reset index for cleaner output\n",
        "pivoted_clean = pivoted.reset_index()\n",
        "pivoted_clean.columns.name = None  # Remove column name\n",
        "print(\"\\nCleaned:\")\n",
        "print(pivoted_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 9: Time Series\n",
        "\n",
        "Pandas has excellent support for time series data:\n",
        "- **DateTime parsing and creation**\n",
        "- **Date components extraction**\n",
        "- **Resampling** (changing frequency)\n",
        "- **Rolling windows** (moving averages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DATETIME HANDLING\n",
        "# ========================================\n",
        "\n",
        "# Create datetime from strings\n",
        "dates = pd.to_datetime(['2024-01-15', '2024-02-20', '2024-03-25'])\n",
        "print(\"Parsed dates:\")\n",
        "print(dates)\n",
        "\n",
        "# Create date range\n",
        "date_range = pd.date_range(start='2024-01-01', periods=7, freq='D')\n",
        "print(\"\\nDate range (7 days):\")\n",
        "print(date_range)\n",
        "\n",
        "# Create DataFrame with datetime index\n",
        "df_ts = pd.DataFrame({\n",
        "    'date': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
        "    'sales': [100, 120, 90, 150, 130, 140, 160, 110, 125, 135]\n",
        "})\n",
        "print(\"\\nTime series DataFrame:\")\n",
        "print(df_ts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# DATE COMPONENTS & RESAMPLING\n",
        "# ========================================\n",
        "\n",
        "# Extract date components\n",
        "df_ts['year'] = df_ts['date'].dt.year\n",
        "df_ts['month'] = df_ts['date'].dt.month\n",
        "df_ts['day'] = df_ts['date'].dt.day\n",
        "df_ts['day_name'] = df_ts['date'].dt.day_name()\n",
        "\n",
        "print(\"=== Date Components ===\")\n",
        "print(df_ts[['date', 'year', 'month', 'day', 'day_name']].head())\n",
        "\n",
        "# Set date as index for resampling\n",
        "df_indexed = df_ts.set_index('date')\n",
        "\n",
        "print(\"\\n=== Rolling Window (3-day moving average) ===\")\n",
        "df_indexed['rolling_avg'] = df_indexed['sales'].rolling(window=3).mean()\n",
        "print(df_indexed[['sales', 'rolling_avg']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 10: Input/Output Operations\n",
        "\n",
        "| Format | Read | Write |\n",
        "|--------|------|-------|\n",
        "| CSV | `pd.read_csv()` | `df.to_csv()` |\n",
        "| Excel | `pd.read_excel()` | `df.to_excel()` |\n",
        "| JSON | `pd.read_json()` | `df.to_json()` |\n",
        "| SQL | `pd.read_sql()` | `df.to_sql()` |\n",
        "| Parquet | `pd.read_parquet()` | `df.to_parquet()` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# READING & WRITING FILES\n",
        "# ========================================\n",
        "\n",
        "# Create sample data\n",
        "sample_df = pd.DataFrame({\n",
        "    'name': ['Alice', 'Bob', 'Charlie'],\n",
        "    'age': [25, 30, 35],\n",
        "    'city': ['NYC', 'LA', 'Chicago']\n",
        "})\n",
        "\n",
        "# === CSV ===\n",
        "# Write to CSV\n",
        "sample_df.to_csv('sample_output.csv', index=False)\n",
        "print(\"âœ… Saved to sample_output.csv\")\n",
        "\n",
        "# Read from CSV\n",
        "# df = pd.read_csv('filename.csv')\n",
        "\n",
        "# Common read_csv parameters:\n",
        "# pd.read_csv('file.csv',\n",
        "#     sep=',',              # Delimiter\n",
        "#     header=0,             # Row number for headers\n",
        "#     index_col=None,       # Column to use as index\n",
        "#     usecols=['col1'],     # Columns to read\n",
        "#     dtype={'col': int},   # Column data types\n",
        "#     na_values=['NA'],     # Values to treat as NaN\n",
        "#     nrows=100,            # Number of rows to read\n",
        "#     skiprows=1            # Rows to skip\n",
        "# )\n",
        "\n",
        "print(\"\\\\nCommon I/O operations demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 11: Performance Optimization\n",
        "\n",
        "## Best Practices for Faster Pandas Code\n",
        "\n",
        "1. **Use vectorized operations** (avoid loops!)\n",
        "2. **Choose appropriate dtypes** (save memory)\n",
        "3. **Use `query()` for filtering** (often faster)\n",
        "4. **Read only needed columns**\n",
        "5. **Use `category` dtype for repeated strings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# PERFORMANCE: VECTORIZATION VS LOOPS\n",
        "# ========================================\n",
        "\n",
        "import time\n",
        "\n",
        "# Create large DataFrame\n",
        "large_df = pd.DataFrame({\n",
        "    'A': np.random.randint(1, 100, 100000),\n",
        "    'B': np.random.randint(1, 100, 100000)\n",
        "})\n",
        "\n",
        "# BAD: Using loop (SLOW!)\n",
        "start = time.time()\n",
        "result_loop = []\n",
        "for i in range(len(large_df)):\n",
        "    result_loop.append(large_df['A'].iloc[i] + large_df['B'].iloc[i])\n",
        "loop_time = time.time() - start\n",
        "\n",
        "# GOOD: Vectorized operation (FAST!)\n",
        "start = time.time()\n",
        "result_vector = large_df['A'] + large_df['B']\n",
        "vector_time = time.time() - start\n",
        "\n",
        "print(f\"Loop time: {loop_time:.4f} seconds\")\n",
        "print(f\"Vectorized time: {vector_time:.4f} seconds\")\n",
        "print(f\"Vectorization is {loop_time/vector_time:.0f}x faster!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# MEMORY OPTIMIZATION\n",
        "# ========================================\n",
        "\n",
        "# Create DataFrame with repeated strings\n",
        "df_mem = pd.DataFrame({\n",
        "    'category': ['A', 'B', 'C'] * 10000,\n",
        "    'value': np.random.randint(1, 100, 30000)\n",
        "})\n",
        "\n",
        "print(\"=== Memory Usage ===\")\n",
        "print(f\"\\nBefore optimization:\")\n",
        "print(df_mem.memory_usage(deep=True))\n",
        "print(f\"Total: {df_mem.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "\n",
        "# Convert to category dtype\n",
        "df_mem['category'] = df_mem['category'].astype('category')\n",
        "\n",
        "print(f\"\\nAfter converting to category:\")\n",
        "print(df_mem.memory_usage(deep=True))\n",
        "print(f\"Total: {df_mem.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "\n",
        "# Downcast numeric types\n",
        "df_mem['value'] = pd.to_numeric(df_mem['value'], downcast='integer')\n",
        "print(f\"\\nAfter downcasting integers:\")\n",
        "print(f\"Total: {df_mem.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Module 12: Real-World Project\n",
        "\n",
        "## ðŸŽ¯ Project: Sales Data Analysis\n",
        "\n",
        "Let's apply everything we've learned to analyze a complete sales dataset!\n",
        "\n",
        "### Objectives:\n",
        "1. Load and explore the data\n",
        "2. Clean missing values and fix data types\n",
        "3. Perform exploratory analysis\n",
        "4. Create summary statistics by category\n",
        "5. Generate insights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 1: CREATE SAMPLE SALES DATA\n",
        "# ========================================\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate realistic sales data\n",
        "n_records = 500\n",
        "sales_data = pd.DataFrame({\n",
        "    'order_id': range(1001, 1001 + n_records),\n",
        "    'date': pd.date_range('2024-01-01', periods=n_records, freq='D'),\n",
        "    'product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard'], n_records),\n",
        "    'category': np.random.choice(['Electronics', 'Accessories'], n_records),\n",
        "    'region': np.random.choice(['North', 'South', 'East', 'West'], n_records),\n",
        "    'quantity': np.random.randint(1, 10, n_records),\n",
        "    'unit_price': np.random.choice([999, 699, 499, 299, 79], n_records),\n",
        "    'customer_id': np.random.randint(100, 200, n_records)\n",
        "})\n",
        "\n",
        "# Add some missing values (realistic scenario)\n",
        "sales_data.loc[np.random.choice(n_records, 20), 'region'] = None\n",
        "sales_data.loc[np.random.choice(n_records, 10), 'unit_price'] = None\n",
        "\n",
        "print(\"=== STEP 1: Raw Data Overview ===\")\n",
        "print(f\"Shape: {sales_data.shape}\")\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "print(sales_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 2: DATA EXPLORATION & CLEANING\n",
        "# ========================================\n",
        "\n",
        "print(\"=== STEP 2: Data Exploration ===\")\n",
        "print(\"\\n--- Data Types ---\")\n",
        "print(sales_data.dtypes)\n",
        "\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "print(sales_data.isna().sum())\n",
        "\n",
        "print(\"\\n--- Unique Values ---\")\n",
        "print(f\"Products: {sales_data['product'].unique()}\")\n",
        "print(f\"Regions: {sales_data['region'].dropna().unique()}\")\n",
        "\n",
        "# Clean the data\n",
        "sales_clean = sales_data.copy()\n",
        "\n",
        "# Fill missing regions with 'Unknown'\n",
        "sales_clean['region'] = sales_clean['region'].fillna('Unknown')\n",
        "\n",
        "# Fill missing prices with median\n",
        "sales_clean['unit_price'] = sales_clean['unit_price'].fillna(\n",
        "    sales_clean['unit_price'].median()\n",
        ")\n",
        "\n",
        "# Calculate total revenue\n",
        "sales_clean['revenue'] = sales_clean['quantity'] * sales_clean['unit_price']\n",
        "\n",
        "print(\"\\nâœ… Data cleaned! Missing values after cleaning:\")\n",
        "print(sales_clean.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 3: ANALYSIS & INSIGHTS\n",
        "# ========================================\n",
        "\n",
        "print(\"=== STEP 3: Sales Analysis ===\")\n",
        "\n",
        "# Overall statistics\n",
        "print(\"\\n--- Overall Revenue Statistics ---\")\n",
        "print(f\"Total Revenue: ${sales_clean['revenue'].sum():,.2f}\")\n",
        "print(f\"Average Order Value: ${sales_clean['revenue'].mean():,.2f}\")\n",
        "print(f\"Total Orders: {len(sales_clean)}\")\n",
        "\n",
        "# Revenue by product\n",
        "print(\"\\n--- Revenue by Product ---\")\n",
        "product_revenue = sales_clean.groupby('product').agg(\n",
        "    total_revenue=('revenue', 'sum'),\n",
        "    avg_revenue=('revenue', 'mean'),\n",
        "    order_count=('order_id', 'count')\n",
        ").sort_values('total_revenue', ascending=False)\n",
        "print(product_revenue)\n",
        "\n",
        "# Revenue by region\n",
        "print(\"\\n--- Revenue by Region ---\")\n",
        "region_revenue = sales_clean.groupby('region')['revenue'].agg(['sum', 'mean', 'count'])\n",
        "region_revenue.columns = ['Total', 'Average', 'Orders']\n",
        "print(region_revenue.sort_values('Total', ascending=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# STEP 4: ADVANCED ANALYSIS\n",
        "# ========================================\n",
        "\n",
        "print(\"=== STEP 4: Advanced Analysis ===\")\n",
        "\n",
        "# Pivot table: Revenue by Region and Product\n",
        "print(\"\\n--- Pivot Table: Revenue by Region Ã— Product ---\")\n",
        "pivot = pd.pivot_table(\n",
        "    sales_clean,\n",
        "    values='revenue',\n",
        "    index='region',\n",
        "    columns='product',\n",
        "    aggfunc='sum',\n",
        "    margins=True\n",
        ")\n",
        "print(pivot.round(0))\n",
        "\n",
        "# Time-based analysis: Monthly revenue\n",
        "sales_clean['month'] = sales_clean['date'].dt.to_period('M')\n",
        "print(\"\\n--- Monthly Revenue Trend ---\")\n",
        "monthly = sales_clean.groupby('month')['revenue'].sum()\n",
        "print(monthly.head(12))\n",
        "\n",
        "# Top customers\n",
        "print(\"\\n--- Top 5 Customers by Revenue ---\")\n",
        "top_customers = sales_clean.groupby('customer_id')['revenue'].sum().nlargest(5)\n",
        "print(top_customers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ðŸŽ“ Summary & Next Steps\n",
        "\n",
        "## What You've Learned\n",
        "\n",
        "| Module | Key Skills |\n",
        "|--------|------------|\n",
        "| **1. Setup** | Installing, importing Pandas |\n",
        "| **2. Data Structures** | Series, DataFrame creation |\n",
        "| **3. Selection** | loc, iloc, boolean indexing |\n",
        "| **4. Cleaning** | Missing data, duplicates, strings |\n",
        "| **5. Transformation** | apply, map, sorting |\n",
        "| **6. Aggregation** | groupby, pivot tables |\n",
        "| **7. Merging** | concat, merge, join |\n",
        "| **8. Reshaping** | melt, pivot, stack |\n",
        "| **9. Time Series** | datetime, resampling, rolling |\n",
        "| **10. I/O** | CSV, Excel, JSON |\n",
        "| **11. Performance** | Vectorization, memory |\n",
        "| **12. Project** | End-to-end analysis |\n",
        "\n",
        "## Practice Exercises\n",
        "\n",
        "1. **Load a CSV file** and perform basic exploration\n",
        "2. **Clean a dataset** with missing values and duplicates\n",
        "3. **Group and aggregate** data to find insights\n",
        "4. **Merge two DataFrames** using different join types\n",
        "5. **Create a pivot table** for sales analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# ðŸ“‹ Pandas Cheat Sheet\n",
        "\n",
        "## Creating Data\n",
        "```python\n",
        "pd.Series([1, 2, 3])                    # Create Series\n",
        "pd.DataFrame({'col': [1, 2, 3]})        # Create DataFrame\n",
        "pd.read_csv('file.csv')                 # Read CSV\n",
        "```\n",
        "\n",
        "## Selection\n",
        "```python\n",
        "df['col']                               # Single column\n",
        "df[['col1', 'col2']]                    # Multiple columns\n",
        "df.loc[row_label, col_label]            # By label\n",
        "df.iloc[row_pos, col_pos]               # By position\n",
        "df[df['col'] > value]                   # Boolean filter\n",
        "```\n",
        "\n",
        "## Cleaning\n",
        "```python\n",
        "df.isna().sum()                         # Count missing\n",
        "df.dropna()                             # Drop missing rows\n",
        "df.fillna(value)                        # Fill missing\n",
        "df.drop_duplicates()                    # Remove duplicates\n",
        "df['col'].astype(int)                   # Convert type\n",
        "```\n",
        "\n",
        "## Transformation\n",
        "```python\n",
        "df['new'] = df['a'] + df['b']           # New column\n",
        "df['col'].apply(func)                   # Apply function\n",
        "df.sort_values('col')                   # Sort\n",
        "df.rename(columns={'old': 'new'})       # Rename\n",
        "```\n",
        "\n",
        "## Aggregation\n",
        "```python\n",
        "df.groupby('col').mean()                # Group and aggregate\n",
        "df.groupby('col').agg(['sum', 'mean'])  # Multiple aggs\n",
        "pd.pivot_table(df, values, index, cols) # Pivot table\n",
        "```\n",
        "\n",
        "## Merging\n",
        "```python\n",
        "pd.concat([df1, df2])                   # Stack vertically\n",
        "pd.merge(df1, df2, on='key')            # SQL-style join\n",
        "```\n",
        "\n",
        "---\n",
        "**ðŸŽ‰ Congratulations! You've completed the Pandas Mastery Course!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
